{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjuAUNVTrwgLZ4VU7zucfM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eba57/hatespeechdetection/blob/main/amharicRANDOMFOREST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkgsLmW1isNi",
        "outputId": "6f65ae75-9dbf-41db-e8d9-7012e757840a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from tensorflow.keras.utils import np_utils\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "data_to_load = files.upload()\n",
        "import io\n",
        "dataset = pd.read_csv(io.BytesIO(data_to_load['amharic.csv']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "r8ouXROcjNAI",
        "outputId": "c16f9d03-44fd-4e22-e150-9813210cd1c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0db9d74a-b996-441a-88a1-00a63b3eafae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0db9d74a-b996-441a-88a1-00a63b3eafae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving amharic.csv to amharic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet=dataset.tweet\n",
        "labels=dataset.labels"
      ],
      "metadata": {
        "id": "uAmKJVxijSWo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['labels']=[re.sub(r\"\\n\", \"\", str(label)) for label in labels]"
      ],
      "metadata": {
        "id": "ynmE5LG4jVmT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop function which is used in removing or deleting rows or columns from the CSV files\n",
        "dataset.drop('Unnamed: 2', inplace=True, axis=1)\n",
        "\n",
        "# display\n",
        "print(\"\\nCSV Data after deleting the column 'Unnamed: 2':\\n\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHYaLS6qjWSY",
        "outputId": "6449d4af-01bc-4c69-850c-7d3be141f3e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CSV Data after deleting the column 'Unnamed: 2':\n",
            "\n",
            "      labels                                              tweet\n",
            "0      Free   አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...\n",
            "1       Free   እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደ...\n",
            "2      Free   የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...\n",
            "3       Free  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...\n",
            "4       Hate  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...\n",
            "...      ...                                                ...\n",
            "29995   Hate         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን\n",
            "29996   Free             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ\n",
            "29997   Free   እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እ...\n",
            "29998   Hate   ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክ...\n",
            "29999   Hate  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...\n",
            "\n",
            "[30000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = dataset['tweet']\n",
        "finalWords = []\n",
        "for word in words:\n",
        "    \n",
        "    \n",
        "    #remove any non-alphanumeric character\n",
        "    word = re.sub(r'\\W', ' ', str(word))\n",
        "    \n",
        "    #remove any digit\n",
        "    word = re.sub(\"(\\\\d)+\",\" \",word)\n",
        "    \n",
        "    #remove punctuation \n",
        "    word = re.sub(\",#'!:;\",\" \",word)\n",
        "    \n",
        "    #remove any latin characters \n",
        "    word = re.sub(r'[a-zA-Z]+', '', word)\n",
        "\n",
        "    word = re.sub('\\s+', ' ', word)  # remove extra whitespace\n",
        "    finalWords.append(word)"
      ],
      "metadata": {
        "id": "m2isCLvMjY64"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizedList = []\n",
        "for lineWord in finalWords:\n",
        "    separatedWords = lineWord.split(' ')\n",
        "    word = [word for word in separatedWords if   word.strip() != '']\n",
        "    tokenizedList.append(word)"
      ],
      "metadata": {
        "id": "CCw77u13jbyL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list amharic stop words\n",
        "stopwords=['ነው', 'እና', 'እኔ', 'የእኔ','እኔ','ራሴ','እኛ','የእኛ','እኛ','ራሳችን','አንቺ' ,'የእርስዎ' ,'እራስዎ','እራሳችሁን','የእሱ',\n",
        "           'እሱ','እ','ራሱ','እሷ' 'ራሷን','እሱ','በራሱ','እነሱ','የእነሱ','እራሳቸው','ምንድን','የት','ማን','ይሄ','ያንን','እነዚህ','እነዚያ'\n",
        "           ,'ነው','እነዚህ','ናቸው','ነበር','ነበሩ','መሆን','ሆኗል','መሆን','አለ','ነበሩ','አለው','መስራት','ያደርገዋል','አደረጉት',\n",
        "           'ማድረግ','እና','ግን','ከሆነ','ወይም','ምክንያቱም','እንደ','እስከ','ገና','የ','ለ','ከ','ጋር','ስለ','መካከል','ውስጥ',\n",
        "           'ከዚህ','በፊት','በኋላ','ከላይ','ከታች','ወደ','ከ','ወደ','ላይ','ታች','ውስጥ','ውጭ','በላይ','በታች','በድጋሚ','እናስ','እናም']\n",
        "\n",
        "           \n",
        "cleanedWordsList= []\n",
        "for lineWords in tokenizedList:\n",
        "    tempList = []\n",
        "    for word in lineWords:\n",
        "        if word.strip() not in stopwords:\n",
        "            tempList.append(word)\n",
        "    cleanedWordsList.append(tempList)"
      ],
      "metadata": {
        "id": "e3S__UuBjeTa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the time being this is fine but convert it to something good but not this at all\n",
        "\n",
        "normalizedSenteces = []\n",
        "for linesWords in cleanedWordsList:\n",
        "    tempList = []\n",
        "    for word in linesWords:\n",
        "        \n",
        "        word = re.sub(\"ሐ\",\"ሀ\",word)\n",
        "        word = re.sub(\"ሑ\",\"ሁ\",word)\n",
        "        word = re.sub(\"ሒ\",\"ሂ\",word)\n",
        "        word = re.sub(\"ሓ\",\"ሃ\",word)\n",
        "        word = re.sub(\"ሔ\",\"ሄ\",word)\n",
        "        word = re.sub(\"ሕ\",\"ህ\",word)\n",
        "        word = re.sub(\"ሖ\",\"ሆ\",word)\n",
        "        \n",
        "        word = re.sub(\"ኀ\",\"ሀ\",word)\n",
        "        word = re.sub(\"ኁ\",\"ሁ\",word)\n",
        "        word = re.sub(\"ኂ\",\"ሂ\",word)\n",
        "        word = re.sub(\"ኃ\",\"ሃ\",word)\n",
        "        word = re.sub(\"ኄ\",\"ሄ\",word)\n",
        "        word = re.sub(\"ኅ\",\"ህ\",word)\n",
        "        word = re.sub(\"ኆ\",\"ሆ\",word)\n",
        "        \n",
        "        word = re.sub(\"ሠ\",\"ሰ\",word)\n",
        "        word = re.sub(\"ሡ\",\"ሱ\",word)\n",
        "        word = re.sub(\"ሢ\",\"ሲ\",word)\n",
        "        word = re.sub(\"ሣ\",\"ሳ\",word)\n",
        "        word = re.sub(\"ሤ\",\"ሴ\",word)\n",
        "        word = re.sub(\"ሥ\",\"ስ\",word)\n",
        "        word = re.sub(\"ሦ\",\"ሶ\",word)\n",
        "        \n",
        "        word = re.sub(\"ዐ\",\"አ\",word)\n",
        "        word = re.sub(\"ዑ\",\"ኡ\",word)\n",
        "        word = re.sub(\"ዒ\",\"ኢ\",word)\n",
        "        word = re.sub(\"ዓ\",\"ኣ\",word)\n",
        "        word = re.sub(\"ዔ\",\"ኤ\",word)\n",
        "        word = re.sub(\"ዕ\",\"እ\",word)\n",
        "        word = re.sub(\"ዖ\",\"ኦ\",word)\n",
        "        \n",
        "        word = re.sub(\"ዐ\",\"አ\",word)\n",
        "        word = re.sub(\"ዑ\",\"ኡ\",word)\n",
        "        word = re.sub(\"ዒ\",\"ኢ\",word)\n",
        "        word = re.sub(\"ዓ\",\"ኣ\",word)\n",
        "        word = re.sub(\"ዔ\",\"ኤ\",word)\n",
        "        word = re.sub(\"ዕ\",\"እ\",word)\n",
        "        word = re.sub(\"ዖ\",\"ኦ\",word)\n",
        "        \n",
        "        word = re.sub(\"ጸ\",\"ፀ\",word)\n",
        "        word = re.sub(\"ጹ\",\"ፁ\",word)\n",
        "        word = re.sub(\"ጺ\",\"ፂ\",word)\n",
        "        word = re.sub(\"ጻ\",\"ፃ\",word)\n",
        "        word = re.sub(\"ጼ\",\"ፄ\",word)\n",
        "        word = re.sub(\"ጽ\",\"ፅ\",word)\n",
        "        word = re.sub(\"ጾ\",\"ፆ\",word)\n",
        "        \n",
        "        tempList.append(word)\n",
        "    normalizedSenteces.append(tempList)"
      ],
      "metadata": {
        "id": "jAT_hURgjgzI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Split into training and testing data\n",
        "x = dataset['tweet']\n",
        "y = dataset['labels']"
      ],
      "metadata": {
        "id": "hlvyXpQUjrqJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizedSenteces  = pd.Series(normalizedSenteces)\n",
        "\n",
        "finalSentences = []\n",
        "for lists in normalizedSenteces:\n",
        "    temp = \"\"\n",
        "    for word in lists:\n",
        "        temp =  temp +\" \"+word\n",
        "    finalSentences.append(temp)\n",
        "\n",
        "    \n",
        "x = pd.Series(finalSentences)"
      ],
      "metadata": {
        "id": "-ZqR4R0hjuZ0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# initial TTS\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=15)\n",
        "\n"
      ],
      "metadata": {
        "id": "xSgyN6aij9Ow"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
      ],
      "metadata": {
        "id": "VchkRMAmkWJ0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "metadata": {
        "id": "amp0az2MkeNc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating tf_idf vectorizer\n",
        "tfidf = TfidfVectorizer( ngram_range=(1,2))"
      ],
      "metadata": {
        "id": "SEWxIOD8kOZN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming tokenized data into sparse matrix format with 20K stored elements\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ],
      "metadata": {
        "id": "YIi4qbSTlUsg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a quick look at the non zero elements\n",
        "non_zero_cols = X_train_tfidf.nnz / float(X_train_tfidf.shape[0])\n",
        "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
        "percent_sparse = 1 - (non_zero_cols / float(X_train_tfidf.shape[1]))\n",
        "print('Percentage of columns containing ZERO: {}'.format(percent_sparse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lcdVumoleSx",
        "outputId": "391699b5-38b6-452f-9df0-a40afeb72c84"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Number of Non-Zero Elements in Vectorized Articles: 31.093958333333333\n",
            "Percentage of columns containing ZERO: 0.9999196708750773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "rf_baseline = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "JjCHpYn2lmNS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "%%time\n",
        "rf_baseline.fit(X_train_tfidf, y_train)\n",
        "rf_test_preds = rf_baseline.predict(X_test_tfidf)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntbCpB1-lncs",
        "outputId": "8e80f8e7-6807-4bc8-de6c-0c557b744265"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1h 26min 7s, sys: 5.64 s, total: 1h 26min 13s\n",
            "Wall time: 1h 26min 29s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " #rf_baseline = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,criterion='gini' ,random_state=42)\n",
        "            # Train the model\n",
        "            \n",
        "           #train and test\n",
        "#from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# If you're working in Jupyter Notebook, include the following so that plots will display:\n",
        "#%matplotlib inline\n",
        "#accuracy_score(y_test, rf_test_preds)\n",
        "\n",
        "#score = (rf_baseline.score(X_test, y_test)*100)\n",
        "           \n",
        "            # Print the results\n",
        "#print(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqo9vgi3ENPg",
        "outputId": "1667c0e1-ded7-459a-d6fe-06279aa10950"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6741666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#precision_score(y_test, rf_test_preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "CsaSSOIMG8Qu",
        "outputId": "aa99f14f-2d21-4883-e4a0-1054252c78a2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-1e8cf3b88716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_test_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import f1_score\n",
        "#print(\"Precision Score : \",precision_score(y_test, rf_test_preds, pos_label='positive' average='micro'))\n",
        "#f1 = f1_score(Y_test, Y_pred)\n",
        "#print(\"Recall Score : \",recall_score(y_test, rf_test_preds,pos_label='positive'average='micro'))"
      ],
      "metadata": {
        "id": "WJyauHEUGZ0U"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the classification report for test data and predictions\n",
        "#print(classification_report(y_test, rf_test_preds))\n",
        "#print(confusion_matrix(y_test, rf_test_preds))"
      ],
      "metadata": {
        "id": "4AKfXPKSIaM9"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}